{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igqIMEgu64-F"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xybQNYCXYu13"
   },
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAVqRfc2KK3"
   },
   "source": [
    "# Some Utility Functions\n",
    "\n",
    "You do not need to modify this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbrcpfYN2I-H"
   },
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_spatial_sequence(split_rate, full_sequence):\n",
    "    counts = len(full_sequence)\n",
    "    len1 = int(counts * split_rate)\n",
    "    len2 = counts - len1\n",
    "    index1 = np.random.choice(full_sequence, len1, replace=False)\n",
    "    index2 = np.setdiff1d(full_sequence, index1)\n",
    "    print('**************************Data Spliting***************************')\n",
    "    print('Spliting Rate: ', split_rate)\n",
    "    print(len1, 'of Dataset1: ',index1)\n",
    "    print(len2, 'of Dataset2: ',index2)\n",
    "    print('**************************Data Spliting***************************')\n",
    "    return index1, index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_sequence = [2, 8, 9, 10, 11, 12, 15, 20, 25, 30, 34, 40, 44]\n",
    "# print(len(spatial_sequence))\n",
    "# random_spatial_sequence(0.7, spatial_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "#     print(batch)\n",
    "#     print(len(batch[0]))\n",
    "    # 从每个样本的字典中获取处理结果、标签和其他数据，并将它们存储在同一个字典中\n",
    "    processed_data = [torch.FloatTensor(sample[\"processed_data\"]) for sample in batch]\n",
    "    label_data = [sample[\"label_data\"] for sample in batch]\n",
    "    \n",
    "    # 将所有需要传递的数据都保存在同一个字典中\n",
    "    batch_dict = {\"processed_data\": torch.stack(processed_data),\n",
    "                  \"label_data\": label_data}\n",
    "    \n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upscaling_sm(sm_bar, sm_bar_sd, ati, ati_bar, ati_bar_sd):\n",
    "    return sm_bar + sm_bar_sd * (ati - ati_bar) / ati_bar_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)                        \n",
    "        with torch.no_grad():                   \n",
    "            pred = model(x)                     \n",
    "            preds.append(pred.detach().cpu())   \n",
    "    preds = torch.cat(preds, dim=0).numpy()  \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqO3lTm78nNO"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMAPDataset(Dataset):\n",
    "    '''\n",
    "    root: root of input data\n",
    "    temperal_sequence: the sequence of valid day\n",
    "    spatial_sequence: the sequence of valid SMAPID\n",
    "    test: the flag to identify if it is the test dataset\n",
    "    '''\n",
    "    def __init__(self, root, temperal_sequence, spatial_sequence):\n",
    "        # variables for input\n",
    "        self.smap = []\n",
    "        self.texture = []\n",
    "        \n",
    "        # variables for output\n",
    "        self.sm = []\n",
    "#         self.smap_unorm = []\n",
    "        self.ati = [] # contains [ati, atim, atisd] in each element\n",
    "        \n",
    "        print('***************************Load data path******************************')\n",
    "        for i in temperal_sequence: # for example: 20151015\n",
    "            print('_______________________________' + str(i) + '_______________________________')\n",
    "            for j in spatial_sequence: # for example: 1\n",
    "                print('_____________________________smap cell: ' + str(j) + '_____________________________')\n",
    "                # add path for input variables\n",
    "                self.smap.append(root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy')\n",
    "                self.texture.append(root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy')\n",
    "                # display adding path\n",
    "                print((root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy'))\n",
    "                print((root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy'))\n",
    "#                 print(os.path.exists(root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy'))\n",
    "#                 print(os.path.exists(root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy'))\n",
    "                      \n",
    "                # one smap to many in-situ sm\n",
    "                smap_to_insitu = np.load(root + \"LABEL\\\\SMAPID2INSITUID\\\\\" + str(j) + '.npy')\n",
    "                insitu_sm_list = []\n",
    "                insitu_ati_list = []\n",
    "                for _id in smap_to_insitu:\n",
    "                    insitu_sm_list.append(root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\")\n",
    "                    insitu_ati_list.append(root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\")\n",
    "                    # display adding path\n",
    "                    print((root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                    print((root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "#                     print(os.path.exists(root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "#                     print(os.path.exists(root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                      \n",
    "                # add the data of insitu in insitu_list\n",
    "                self.sm.append(insitu_sm_list)\n",
    "                self.ati.append(insitu_ati_list)    \n",
    "                      \n",
    "    def __getitem__(self, idx):\n",
    "        smap_path = self.smap[idx]\n",
    "        texture_path = self.texture[idx]\n",
    "        smap = np.load(smap_path)\n",
    "        texture = np.load(texture_path)\n",
    "        \n",
    "        data_pkg = {'processed_data': [], 'label_data': []}\n",
    "        \n",
    "        # choose flatten as the way to concatenate the input feature\n",
    "        x = self.__flatten__(smap, texture)\n",
    "        data_pkg['processed_data'] = x\n",
    "        \n",
    "        sm_list = self.sm[idx]\n",
    "        ati_list = self.ati[idx]\n",
    "        y = [] # y -> [[sm, smap, ati], ...], sm -> [float], smap -> [float],1 ati -> [ati, atim, atisd]\n",
    "        for i in range(len(sm_list)):\n",
    "            sm_path = sm_list[i]\n",
    "            ati_path = ati_list[i]\n",
    "            sm = np.load(sm_path, allow_pickle=True)\n",
    "            ati = np.load(ati_path, allow_pickle=True)\n",
    "            data_pkg['label_data'].append([sm, smap, ati])      # other_data -> [[sm, smap, ati], ...], \n",
    "                                                                    # sm -> [float]\n",
    "                                                                    # smap -> [float], \n",
    "                                                                    # ati -> [ati, atim, atisd]\n",
    "            \n",
    "            # test: each element in list of batch should be of equal size\n",
    "#             break\n",
    "        return data_pkg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smap)\n",
    "\n",
    "    ### the way to concatenate input data\n",
    "    def __flatten__(self, smap, texture):\n",
    "        # normalization is done before loading\n",
    "        texture_flat = texture.flatten()\n",
    "        return  np.concatenate((smap, texture_flat), axis=0)\n",
    "    \n",
    "    def get_input_shape(self, idx):\n",
    "        data_pkg = self.__getitem__(0)\n",
    "        return data_pkg['processed_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten(smap, texture):\n",
    "#     texture_flat = np.ravel(texture)\n",
    "#     print(texture_flat)\n",
    "#     return  np.concatenate((smap, texture_flat), axis=0)\n",
    "\n",
    "# root = 'E:\\\\downscaling\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\INPUT\\\\'\n",
    "# smap = np.load(root + 'SMAP\\\\20151015\\\\0.npy')\n",
    "# texture = np.load(root + 'TEXTURE\\\\0.npy')\n",
    "# print(smap.shape)\n",
    "# print(texture.shape)\n",
    "# print(flatten(smap, texture).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = 'E:\\\\downscaling\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\'\n",
    "# directory = root + 'LABEL\\\\SMAPID2INSITUID\\\\'\n",
    "# full_spatial_sequence_smap = sorted([int(f.split('.')[0]) for f in os.listdir(directory) if f.endswith('.npy')]) # !!!! read out of order\n",
    "# print(len(full_spatial_sequence_smap), 'of Full Spatial Sequence: ', full_spatial_sequence_smap)\n",
    "\n",
    "# train_spatial_seq, test_spatial_seq = random_spatial_sequence(0.7, full_spatial_sequence_smap)\n",
    "# train_spatial_seq, valid_spatial_seq = random_spatial_sequence(0.8, train_spatial_seq)\n",
    "\n",
    "# temperal_seq = ['20151015']\n",
    "# train_dataset = SMAPDataset(root, temperal_seq, train_spatial_seq)\n",
    "# valid_dataset = SMAPDataset(root, temperal_seq, valid_spatial_seq)\n",
    "# test_dataset = SMAPDataset(root, temperal_seq, test_spatial_seq)\n",
    "# train_dataset.get_input_shape()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m73ooU75CL_j"
   },
   "source": [
    "# Neural Network Model\n",
    "Try out different model architectures by modifying the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn97_WvvrEkG"
   },
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions. \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m73ooU75CL_j"
   },
   "source": [
    "# Self-defined Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Loss(pred, label_data):\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    loss = 0\n",
    "    for i, y in enumerate(pred):      # each calculation in a batch\n",
    "        # calculate the high resolution sm for each in situ\n",
    "        for situ_pkg in label_data[i]: # each calculation in a smap\n",
    "            sm_situ = situ_pkg[0][0]\n",
    "            sm_bar = situ_pkg[1][0]\n",
    "            ati =  situ_pkg[2][0]\n",
    "            atim = situ_pkg[2][1]\n",
    "            atisd = situ_pkg[2][2]\n",
    "            sm_pred = calculate_upscaling_sm(sm_bar, y, ati, atim, atisd)\n",
    "            _loss = criterion(sm_pred, torch.FloatTensor([sm_situ]))\n",
    "            loss = (loss + _loss)\n",
    "    loss = loss / config['batch_size']\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4Rq8_TztAhq"
   },
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = my_Loss # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9) \n",
    "\n",
    "    if not os.path.isdir(config['root'] + '\\\\OUTPUT\\\\MODELS\\\\'):\n",
    "        os.mkdir(config['root'] + '\\\\OUTPUT\\\\MODELS\\\\') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        for data_pkg in train_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "#             x, y = x.to(device), y.to(device)   # Move your data to device. \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, data_pkg['label_data'])\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "            \n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "#         print('Mean Loss for Epoch {}: {}'.format(epoch, mean_train_loss))\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for data_pkg in valid_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "#             x, y = x.to(device), y.to(device)   # Move your data to device. \n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, data_pkg['label_data'])\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['root'] + '\\\\OUTPUT\\\\MODELS\\\\' + str(best_loss) + '.ckpt') # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pgkOh2e9UjE"
   },
   "source": [
    "# Configurations\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoWPUahCtoT6"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'test_ratio': 0.3,\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 30,     # Number of epochs.            \n",
    "    'batch_size': 5, \n",
    "    'learning_rate': 1e-5,              \n",
    "    'early_stop': 10,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'root': 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrS-aJJh9XkW"
   },
   "source": [
    "# Dataloader\n",
    "Read data from files and set up training, validation, and testing sets. You do not need to modify this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jc7ZfDot2t9",
    "outputId": "62d291db-61c8-424d-ec87-2cfce6dd330d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "# Get the id sequence of all valid SMAP\n",
    "directory = config['root'] + 'LABEL\\\\SMAPID2INSITUID\\\\'\n",
    "full_spatial_sequence_smap = sorted([int(f.split('.')[0]) for f in os.listdir(directory) if f.endswith('.npy')]) # !!!! read out of order\n",
    "print(len(full_spatial_sequence_smap), 'of Full Spatial Sequence: ', full_spatial_sequence_smap)\n",
    "\n",
    "# Split the train\\valid\\test dataset by spatial dimension\n",
    "train_spatial_seq, test_spatial_seq = random_spatial_sequence(1-config['test_ratio'], full_spatial_sequence_smap)\n",
    "train_spatial_seq, valid_spatial_seq = random_spatial_sequence(1-config['valid_ratio'], train_spatial_seq)\n",
    "\n",
    "# Initialize the dataset\n",
    "temperal_seq = ['20151015']\n",
    "train_dataset = SMAPDataset(config['root'], temperal_seq, train_spatial_seq)\n",
    "valid_dataset = SMAPDataset(config['root'], temperal_seq, valid_spatial_seq)\n",
    "test_dataset = SMAPDataset(config['root'], temperal_seq, test_spatial_seq)\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OBYgjCA-YwD"
   },
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdttVRkAfu2t",
    "outputId": "864a1214-e69e-4197-dda0-ce2f6fe07d3c"
   },
   "outputs": [],
   "source": [
    "model = My_Model(input_dim=train_dataset.get_input_shape(0)[0]).to(device) # put your model and data on the same computation device.\n",
    "print(train_dataset.get_input_shape(0)[0])\n",
    "print(model)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhAHGqC9-woK"
   },
   "source": [
    "# Testing\n",
    "The predictions of your model on testing set will be stored at `pred.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5eVdpbvAlAe",
    "outputId": "53d0a7b8-20ed-4f83-8a9b-d860f16aed31"
   },
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "model = My_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device) \n",
    "save_pred(preds, 'pred.csv')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ_k5rY0GvSV"
   },
   "source": [
    "# Reference\n",
    "This notebook uses code written by Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

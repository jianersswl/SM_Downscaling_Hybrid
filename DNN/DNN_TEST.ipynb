{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igqIMEgu64-F"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xybQNYCXYu13"
   },
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAVqRfc2KK3"
   },
   "source": [
    "# Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RbrcpfYN2I-H"
   },
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_spatial_sequence(split_rate, full_sequence):\n",
    "    counts = len(full_sequence)\n",
    "    len1 = int(counts * split_rate)\n",
    "    len2 = counts - len1\n",
    "    index1 = np.random.choice(full_sequence, len1, replace=False)\n",
    "    index2 = np.setdiff1d(full_sequence, index1)\n",
    "    print('**************************Data Spliting***************************')\n",
    "    print('Spliting Rate: ', split_rate)\n",
    "    print(len1, 'of Dataset1: ',index1)\n",
    "    print(len2, 'of Dataset2: ',index2)\n",
    "    print('**************************Data Spliting***************************')\n",
    "    return index1, index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_sequence = [2, 8, 9, 10, 11, 12, 15, 20, 25, 30, 34, 40, 44]\n",
    "# print(len(spatial_sequence))\n",
    "# random_spatial_sequence(0.7, spatial_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "#     print(batch)\n",
    "#     print(len(batch[0]))\n",
    "    # 从每个样本的字典中获取处理结果、标签和其他数据，并将它们存储在同一个字典中\n",
    "    processed_data = [torch.FloatTensor(sample[\"processed_data\"]) for sample in batch]\n",
    "    label_data = [sample[\"label_data\"] for sample in batch]\n",
    "    \n",
    "    # 将所有需要传递的数据都保存在同一个字典中\n",
    "    batch_dict = {\"processed_data\": torch.stack(processed_data),\n",
    "                  \"label_data\": label_data}\n",
    "    \n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upscaling_sm(sm_bar, sm_bar_sd, ati, ati_bar, ati_bar_sd):\n",
    "    return sm_bar + sm_bar_sd * (ati - ati_bar) / ati_bar_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)                        \n",
    "        with torch.no_grad():                   \n",
    "            pred = model(x)                     \n",
    "            preds.append(pred.detach().cpu())   \n",
    "    preds = torch.cat(preds, dim=0).numpy()  \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqO3lTm78nNO"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMAPDataset(Dataset):\n",
    "    '''\n",
    "    root: root of input data\n",
    "    temperal_sequence: the sequence of valid day\n",
    "    spatial_sequence: the sequence of valid SMAPID\n",
    "    test: the flag to identify if it is the test dataset\n",
    "    '''\n",
    "    def __init__(self, root, temperal_sequence, spatial_sequence):\n",
    "        # variables for input\n",
    "        self.smap = []\n",
    "        self.texture = []\n",
    "        \n",
    "        # variables for output\n",
    "        self.sm = []\n",
    "        self.smap_unorm = []\n",
    "        self.ati = [] # contains [ati, atim, atisd] in each element\n",
    "        \n",
    "        print('***************************Load data path******************************')\n",
    "        for i in temperal_sequence: # for example: 2015187\n",
    "            print('_______________________________' + str(i) + '_______________________________')\n",
    "            for j in spatial_sequence: # for example: 1\n",
    "                print('_____________________________smap cell: ' + str(j) + '_____________________________')\n",
    "                # add path for input variables\n",
    "                self.smap.append(root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy')\n",
    "                self.texture.append(root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy')\n",
    "                # display adding path\n",
    "                print((root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy'))\n",
    "                print((root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy'))\n",
    "                print(os.path.exists(root + 'INPUT\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy'))\n",
    "                print(os.path.exists(root + 'INPUT\\\\TEXTURE\\\\' + str(j) + '.npy'))\n",
    "                      \n",
    "                # one smap to many in-situ sm\n",
    "                self.smap_unorm.append(root + 'LABEL\\\\SMAP\\\\' + i + '\\\\' + str(j) + '.npy')\n",
    "                smap_to_insitu = np.load(root + \"LABEL\\\\SMAPID2INSITUID\\\\\" + str(j) + '.npy')\n",
    "                insitu_sm_list = []\n",
    "                insitu_ati_list = []\n",
    "                for _id in smap_to_insitu:\n",
    "                    insitu_sm_list.append(root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\")\n",
    "                    insitu_ati_list.append(root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\")\n",
    "                    # display adding path\n",
    "                    print((root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                    print((root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                    print(os.path.exists(root + \"LABEL\\\\SM\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                    print(os.path.exists(root + \"LABEL\\\\ATI\\\\\" + i + \"\\\\\" + str(_id) + \".npy\"))\n",
    "                      \n",
    "                # add the data of insitu in insitu_list\n",
    "                self.sm.append(insitu_sm_list)\n",
    "                self.ati.append(insitu_ati_list)    \n",
    "                      \n",
    "    def __getitem__(self, idx):\n",
    "        smap_path = self.smap[idx]\n",
    "        smap_unorm_path = self.smap_unorm[idx]\n",
    "        texture_path = self.texture[idx]\n",
    "        smap = np.load(smap_path)\n",
    "        smap_unorm = np.load(smap_unorm_path)\n",
    "        texture = np.load(texture_path)\n",
    "        \n",
    "        data_pkg = {'processed_data': [], 'label_data': []}\n",
    "        \n",
    "        # choose flatten as the way to concatenate the input feature\n",
    "        x = self.__flatten__(smap, texture)\n",
    "        data_pkg['processed_data'] = x\n",
    "        \n",
    "        sm_list = self.sm[idx]\n",
    "        ati_list = self.ati[idx]\n",
    "        y = [] # y -> [[sm, smap, ati], ...], sm -> [float], smap -> [float],1 ati -> [ati, atim, atisd]\n",
    "        for i in range(len(sm_list)):\n",
    "            sm_path = sm_list[i]\n",
    "            ati_path = ati_list[i]\n",
    "            sm = np.load(sm_path, allow_pickle=True)\n",
    "            ati = np.load(ati_path, allow_pickle=True)\n",
    "            data_pkg['label_data'].append([sm, smap_unorm, ati])      # other_data -> [[sm, smap, ati], ...], \n",
    "                                                                    # sm -> [float]\n",
    "                                                                    # smap -> [float], \n",
    "                                                                    # ati -> [ati, atim, atisd]\n",
    "            \n",
    "            # test: each element in list of batch should be of equal size\n",
    "#             break\n",
    "        return data_pkg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smap)\n",
    "\n",
    "    ### the way to concatenate input data\n",
    "    def __flatten__(self, smap, texture):\n",
    "        # normalization is done before loading\n",
    "        texture_flat = texture.flatten()\n",
    "        return  np.concatenate((smap, texture_flat), axis=0)\n",
    "    \n",
    "    def get_input_shape(self, idx):\n",
    "        data_pkg = self.__getitem__(0)\n",
    "        return data_pkg['processed_data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m73ooU75CL_j"
   },
   "source": [
    "# Neural Network Model\n",
    "Try out different model architectures by modifying the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qn97_WvvrEkG"
   },
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions. \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m73ooU75CL_j"
   },
   "source": [
    "# Self-defined Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Loss(pred, label_data):\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    loss = 0\n",
    "    for i, y in enumerate(pred):      # each calculation in a batch\n",
    "        # calculate the high resolution sm for each in situ\n",
    "        for situ_pkg in label_data[i]: # each calculation in a smap\n",
    "            sm_situ = situ_pkg[0][0]\n",
    "            sm_bar = situ_pkg[1][0]\n",
    "            ati =  situ_pkg[2][0]\n",
    "            atim = situ_pkg[2][1]\n",
    "            atisd = situ_pkg[2][2]\n",
    "            sm_pred = calculate_upscaling_sm(sm_bar, y, ati, atim, atisd)\n",
    "#             _loss = criterion(sm_pred, torch.FloatTensor([sm_situ]))\n",
    "            _loss = criterion(sm_pred, torch.tensor(sm_situ, dtype=torch.float32))\n",
    "            loss = (loss + _loss)\n",
    "    loss = loss / config['batch_size']\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k4Rq8_TztAhq"
   },
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = my_Loss # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9) \n",
    "\n",
    "    if not os.path.isdir(config['root'] + '\\\\OUTPUT\\\\MODELS\\\\'):\n",
    "        os.mkdir(config['root'] + '\\\\OUTPUT\\\\MODELS\\\\') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        for data_pkg in train_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "#             x, y = x.to(device), y.to(device)   # Move your data to device. \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, data_pkg['label_data'])\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "            \n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "#         print('Mean Loss for Epoch {}: {}'.format(epoch, mean_train_loss))\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for data_pkg in valid_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "#             x, y = x.to(device), y.to(device)   # Move your data to device. \n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, data_pkg['label_data'])\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['root'] + '\\\\OUTPUT\\\\MODELS\\\\' + str(best_loss) + '.ckpt') # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pgkOh2e9UjE"
   },
   "source": [
    "# Configurations\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QoWPUahCtoT6"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 52013141,      # Your seed number, you can pick your lucky number. :)\n",
    "    'test_ratio': 0.2,\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 300,     # Number of epochs.            \n",
    "    'batch_size': 5, \n",
    "    'learning_rate': 1e-5,              \n",
    "    'early_stop': 10,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'root': 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrS-aJJh9XkW"
   },
   "source": [
    "# Dataloader\n",
    "Read data from files and set up training, validation, and testing sets. You do not need to modify this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "same_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 of Full Spatial Sequence:  [9, 10, 15, 16, 17, 18, 20, 26, 27, 29, 33, 40, 43, 44, 45, 51]\n"
     ]
    }
   ],
   "source": [
    "# Get the id sequence of all valid SMAP 不同日期的SMAPID2INSITUID不同\n",
    "directory = config['root'] + 'LABEL\\\\SMAPID2INSITUID\\\\2015187'\n",
    "full_spatial_sequence_smap = sorted([int(f.split('.')[0]) for f in os.listdir(directory) if f.endswith('.npy')]) # !!!! read out of order\n",
    "print(len(full_spatial_sequence_smap), 'of Full Spatial Sequence: ', full_spatial_sequence_smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jc7ZfDot2t9",
    "outputId": "62d291db-61c8-424d-ec87-2cfce6dd330d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Data Spliting***************************\n",
      "Spliting Rate:  0.8\n",
      "12 of Dataset1:  [10 26 44 29 43 16  9 33 45 40 51 17]\n",
      "4 of Dataset2:  [15 18 20 27]\n",
      "**************************Data Spliting***************************\n",
      "**************************Data Spliting***************************\n",
      "Spliting Rate:  0.8\n",
      "9 of Dataset1:  [17 43 45 40 16 51 29 26 33]\n",
      "3 of Dataset2:  [ 9 10 44]\n",
      "**************************Data Spliting***************************\n",
      "***************************Load data path******************************\n",
      "_______________________________20151015_______________________________\n",
      "_____________________________smap cell: 17_____________________________\n",
      "D:\\1GRADUATED\\paper\\downscaling_data\\Soil_moisture_downscale_czt\\DATASET\\INPUT\\SMAP\\20151015\\17.npy\n",
      "D:\\1GRADUATED\\paper\\downscaling_data\\Soil_moisture_downscale_czt\\DATASET\\INPUT\\TEXTURE\\17.npy\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\LABEL\\\\SMAPID2INSITUID\\\\17.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28232/3896758750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Initialize the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtemperal_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'20151015'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMAPDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperal_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_spatial_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMAPDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperal_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_spatial_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMAPDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperal_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_spatial_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28232/275129012.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, temperal_sequence, spatial_sequence)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;31m# one smap to many in-situ sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmap_unorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'LABEL\\\\SMAP\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0msmap_to_insitu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"LABEL\\\\SMAPID2INSITUID\\\\\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0minsitu_sm_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0minsitu_ati_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\datamining\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\LABEL\\\\SMAPID2INSITUID\\\\17.npy'"
     ]
    }
   ],
   "source": [
    "# Split the train\\valid\\test dataset by spatial dimension\n",
    "train_spatial_seq, test_spatial_seq = random_spatial_sequence(1-config['test_ratio'], full_spatial_sequence_smap)\n",
    "train_spatial_seq, valid_spatial_seq = random_spatial_sequence(1-config['valid_ratio'], train_spatial_seq)\n",
    "\n",
    "# Initialize the dataset\n",
    "temperal_seq = ['20151015']\n",
    "train_dataset = SMAPDataset(config['root'], temperal_seq, train_spatial_seq)\n",
    "valid_dataset = SMAPDataset(config['root'], temperal_seq, valid_spatial_seq)\n",
    "test_dataset = SMAPDataset(config['root'], temperal_seq, test_spatial_seq)\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练集sm站点数据\n",
    "sm = [x['label_data'][0][0] for x in train_dataset]\n",
    "x = [x for x in range(len(sm))]\n",
    "y = sm\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(x, y, c='red', s=10, alpha=1)\n",
    "\n",
    "# 添加标签和标题\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('SM In-Situ')\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n",
    "\n",
    "# 训练集SMAP数据\n",
    "smap = [x['label_data'][0][1] for x in train_dataset]\n",
    "x = [x for x in range(len(sm))]\n",
    "y = smap\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(x, y, c='red', s=10, alpha=1)\n",
    "\n",
    "# 添加标签和标题\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('SMAP')\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 创建数据\n",
    "ati = [x['label_data'][0][2] for x in train_dataset]\n",
    "print(ati)\n",
    "x = [x for x in range(len(ati))]\n",
    "\n",
    "y1 = [x[0] for x in ati]\n",
    "y2 = [x[1] for x in ati]\n",
    "y3 = [x[2] for x in ati]\n",
    "\n",
    "# 创建子图\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "\n",
    "# 在第一个子图中绘制ATI\n",
    "ax1.scatter(x, y1)\n",
    "ax1.set_title('ATI')\n",
    "ax1.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 在第二个子图中绘制ATIM\n",
    "ax2.scatter(x, y2)\n",
    "ax2.set_title('ATIM')\n",
    "ax2.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 在第三个子图中绘制ATISD\n",
    "ax3.scatter(x, y3)\n",
    "ax3.set_title('ATISD')\n",
    "ax3.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集sm站点数据\n",
    "sm = [x['label_data'][0][0] for x in valid_dataset]\n",
    "x = [x for x in range(len(sm))]\n",
    "y = sm\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(x, y, c='red', s=10, alpha=1)\n",
    "\n",
    "# 添加标签和标题\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('SM In-Situ')\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n",
    "\n",
    "# 验证集SMAP数据\n",
    "smap = [x['label_data'][0][1] for x in valid_dataset]\n",
    "x = [x for x in range(len(sm))]\n",
    "y = smap\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(x, y, c='red', s=10, alpha=1)\n",
    "\n",
    "# 添加标签和标题\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('SMAP')\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 创建数据\n",
    "ati = [x['label_data'][0][2] for x in valid_dataset]\n",
    "print(ati)\n",
    "x = [x for x in range(len(ati))]\n",
    "\n",
    "y1 = [x[0] for x in ati]\n",
    "y2 = [x[1] for x in ati]\n",
    "y3 = [x[2] for x in ati]\n",
    "\n",
    "# 创建子图\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "\n",
    "# 在第一个子图中绘制sin函数\n",
    "ax1.scatter(x, y1)\n",
    "ax1.set_title('ATI')\n",
    "ax1.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 在第二个子图中绘制cos函数\n",
    "ax2.scatter(x, y2)\n",
    "ax2.set_title('ATIM')\n",
    "ax2.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 在第三个子图中绘制tan函数\n",
    "ax3.scatter(x, y3)\n",
    "ax3.set_title('ATISD')\n",
    "ax3.set_ylim([-0.02,0.05])\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OBYgjCA-YwD"
   },
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdttVRkAfu2t",
    "outputId": "864a1214-e69e-4197-dda0-ce2f6fe07d3c"
   },
   "outputs": [],
   "source": [
    "model = My_Model(input_dim=train_dataset.get_input_shape(0)[0]).to(device) # put your model and data on the same computation device.\n",
    "print(train_dataset.get_input_shape(0)[0])\n",
    "print(model)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhAHGqC9-woK"
   },
   "source": [
    "# Testing\n",
    "The predictions of your model on testing set will be stored at `pred.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5eVdpbvAlAe",
    "outputId": "53d0a7b8-20ed-4f83-8a9b-d860f16aed31"
   },
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "model = My_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device) \n",
    "save_pred(preds, 'pred.csv')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ_k5rY0GvSV"
   },
   "source": [
    "# Reference\n",
    "This notebook uses code written by Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

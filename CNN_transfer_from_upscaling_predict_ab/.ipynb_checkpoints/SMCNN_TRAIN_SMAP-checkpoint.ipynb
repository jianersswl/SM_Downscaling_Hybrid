{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igqIMEgu64-F"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xybQNYCXYu13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jianer\\anaconda3\\envs\\datamining\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ab_physics_loss_smap' from 'SDLoss' (D:\\1GRADUATED\\paper\\ZResearch\\downscaling\\code\\SM_Downscaling_Hybrid\\CNN_transfer_from_upscaling_predict_ab\\SDLoss.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24060/2037294078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSMAPDataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMAPDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSMCNN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSDLoss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mself_defined_smap_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mab_physics_loss_smap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom_spatial_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_insitu_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ab_physics_loss_smap' from 'SDLoss' (D:\\1GRADUATED\\paper\\ZResearch\\downscaling\\code\\SM_Downscaling_Hybrid\\CNN_transfer_from_upscaling_predict_ab\\SDLoss.py)"
     ]
    }
   ],
   "source": [
    "# Reading/Writing Data\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Self-Defined Package\n",
    "from SMAPDataset import SMAPDataset\n",
    "from SMCNN import SMCNN\n",
    "from SDLoss import self_defined_smap_loss\n",
    "from Preprocessing import random_spatial_sequence, collate_fn, collate_insitu_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAVqRfc2KK3"
   },
   "source": [
    "# Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbrcpfYN2I-H"
   },
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pgkOh2e9UjE"
   },
   "source": [
    "# Configurations\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoWPUahCtoT6"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 11611801,      # Your seed number, you can pick your lucky number. :)\n",
    "    'is_train': True,\n",
    "    'test_ratio': 0.2,\n",
    "    'valid_ratio': 0.1,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 5000,     # Number of epochs.            \n",
    "    'train_batch_size': 71*2, \n",
    "    'valid_batch_size': 78,\n",
    "    'test_batch_size': 197,\n",
    "    'sim_threshold': 0.8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'step_size': 10,\n",
    "    'gamma': 0.5,\n",
    "    'momentum': 0.9,\n",
    "    'early_stop': 50,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'root': 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\AB',\n",
    "    'model_save_dir': 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\AB\\\\OUTPUT\\\\MODELS\\\\CNN_FROM_UPSCALING'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrS-aJJh9XkW"
   },
   "source": [
    "# Dataloader\n",
    "Read data from files and set up training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set seed for reproducibility\n",
    "same_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "dataset = SMAPDataset(config['root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集和测试集的长度\n",
    "train_len = int(len(dataset) * (1-config['test_ratio']))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "# 使用 random_split 函数进行划分\n",
    "train_dataset, test_dataset = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "# 计算训练集和验证集的长度\n",
    "valid_len = int(train_len * (config['valid_ratio']))\n",
    "train_len = train_len - valid_len\n",
    "\n",
    "# 使用 random_split 函数进行划分\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_len, valid_len])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['test_batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('training size', len(train_dataset))\n",
    "print('validing size', len(valid_dataset))\n",
    "print('testing size', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4Rq8_TztAhq"
   },
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "    relu = torch.nn.ReLU(inplace=True)\n",
    "    criterion = self_defined_smap_loss \n",
    "    optimizer = torch.optim.SGD(model.parameters(), weight_decay=0.01, lr=config['learning_rate'], momentum=config['momentum']) \n",
    "    \n",
    "    # learning rate decay\n",
    "    RLRP = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=config['gamma'], patience=config['step_size'], threshold=0.03)\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch==35:\n",
    "            config['sim_threshold'] = 0.93\n",
    "        model.train() # Set your model to train mode.\n",
    "        \n",
    "        loss_record = []\n",
    "        physical_loss_record = []\n",
    "        sim_loss_record = []\n",
    "        for data_pkg in train_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "            optimizer.zero_grad()               \n",
    "            pred = model(x)\n",
    "            relu(pred[:, 0])\n",
    "#             print(pred)\n",
    "#             x, pred = x.to(device), pred.to(device)  \n",
    "            physical_loss, sim_loss = criterion(pred, data_pkg['label_data'], 'Training', config['sim_threshold'])\n",
    "            loss = physical_loss + sim_loss\n",
    "            loss.backward()                     \n",
    "            optimizer.step()                   \n",
    "            step += 1\n",
    "            \n",
    "            physical_loss_record.append(physical_loss.detach().item())\n",
    "            sim_loss_record.append(sim_loss.detach().item())\n",
    "            loss_record.append(loss.detach().item())\n",
    "         \n",
    "        mean_train_physical_loss = sum(physical_loss_record)/len(physical_loss_record)\n",
    "        mean_train_sim_loss = sum(sim_loss_record)/len(sim_loss_record)\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "\n",
    "        model.eval() \n",
    "        loss_record = []\n",
    "        physical_loss_record = []\n",
    "        sim_loss_record = []\n",
    "        for data_pkg in valid_loader:\n",
    "            x = data_pkg['processed_data']\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                relu(pred[:, 0])\n",
    "#             x, pred = x.to(device), pred.to(device)  \n",
    "                physical_loss, sim_loss = criterion(pred, data_pkg['label_data'], 'Validing', config['sim_threshold'])\n",
    "                loss = physical_loss + sim_loss\n",
    "            \n",
    "            physical_loss_record.append(physical_loss.item())\n",
    "            sim_loss_record.append(sim_loss.item())\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_physical_loss = sum(physical_loss_record)/len(physical_loss_record)\n",
    "        mean_valid_sim_loss = sum(sim_loss_record)/len(sim_loss_record)\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        \n",
    "        current_lr = (optimizer.param_groups[0])['lr']\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}] (LR: {current_lr}):')\n",
    "        print(f'Train loss: {mean_train_loss:.4f} || Train physical loss: {mean_train_physical_loss:.4f} || Train similarity loss: {mean_train_sim_loss:.4f}')\n",
    "        print(f'Valid loss: {mean_valid_loss:.4f} || Valid physical loss: {mean_valid_physical_loss:.4f} || Valid similarity loss: {mean_valid_sim_loss:.4f}')\n",
    "            \n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            _name = str(best_loss)\n",
    "            _name = 'first'\n",
    "            torch.save(model.state_dict(), os.path.join(config['model_save_dir'], _name + '.ckpt')) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            print(os.path.join(config['model_save_dir'], _name + '.ckpt'))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            _name = str(best_loss)\n",
    "            _name = 'last'\n",
    "            torch.save(model.state_dict(), os.path.join(config['model_save_dir'], _name + '.ckpt')) # Save your best model\n",
    "            return\n",
    "        #         StepLR.step()\n",
    "        RLRP.step(mean_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OBYgjCA-YwD"
   },
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdttVRkAfu2t",
    "outputId": "864a1214-e69e-4197-dda0-ce2f6fe07d3c"
   },
   "outputs": [],
   "source": [
    "# 创建模型保存目录\n",
    "if os.path.exists(config['model_save_dir'])==False:\n",
    "    os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "    \n",
    "if config['is_train']==True:\n",
    "    print(dataset.get_input_shape(0))\n",
    "    model = SMCNN(input_channel=dataset.get_input_shape(0)[2]).to(device) # put your model and data on the same computation device.\n",
    "    print(model)\n",
    "    trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(test_loader, model, config, device):\n",
    "    relu = torch.nn.ReLU(inplace=True)\n",
    "    criterion = self_defined_smap_loss\n",
    "    model.eval() \n",
    "    loss_record = []\n",
    "    physical_loss_record = []\n",
    "    sim_loss_record = []\n",
    "    for data_pkg in test_loader:\n",
    "        x = data_pkg['processed_data']\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            relu(pred[:, 0])\n",
    "#             x, pred = x.to(device), pred.to(device)  \n",
    "            physical_loss, sim_loss = criterion(pred, data_pkg['label_data'], 'Validing', config['sim_threshold'])\n",
    "            loss = physical_loss + sim_loss\n",
    "\n",
    "        physical_loss_record.append(physical_loss.item())\n",
    "        sim_loss_record.append(sim_loss.item())\n",
    "        loss_record.append(loss.item())\n",
    "\n",
    "    mean_test_physical_loss = sum(physical_loss_record)/len(physical_loss_record)\n",
    "    mean_test_sim_loss = sum(sim_loss_record)/len(sim_loss_record)\n",
    "    mean_test_loss = sum(loss_record)/len(loss_record)\n",
    "\n",
    "    print(f'Test loss: {mean_test_loss:.4f} || Valid physical loss: {mean_test_physical_loss:.4f} || Valid similarity loss: {mean_test_sim_loss:.4f}')\n",
    "    return loss_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用smap数据集检验预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_path = os.path.join(config['model_save_dir'],'first.ckpt' )\n",
    "print(dataset.get_input_shape(0))\n",
    "model = SMCNN(input_channel=dataset.get_input_shape(0)[2]).to(device)\n",
    "print(model)\n",
    "model.load_state_dict(torch.load(param_path))\n",
    "loss_record = tester(test_loader, model, config, device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用SMAP数据集检验迁移模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_model_root = 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\AB\\\\OUTPUT\\\\MODELS\\\\CNN_TRANSFER'\n",
    "param_path = os.path.join(transf_model_root,'first.ckpt' )\n",
    "print(dataset.get_input_shape(0))\n",
    "model = SMCNN(input_channel=dataset.get_input_shape(0)[2]).to(device)\n",
    "print(model)\n",
    "model.load_state_dict(torch.load(param_path))\n",
    "loss_record = tester(test_loader, model, config, device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用SMAP数据集检验站点数据单独训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insitu_model_root = 'D:\\\\1GRADUATED\\\\paper\\\\downscaling_data\\\\Soil_moisture_downscale_czt\\\\DATASET\\\\AB\\\\OUTPUT\\\\MODELS\\\\CNN_FROM_INSITU'\n",
    "param_path = os.path.join(insitu_model_root,'first.ckpt' )\n",
    "print(dataset.get_input_shape(0))\n",
    "model = SMCNN(input_channel=dataset.get_input_shape(0)[2]).to(device)\n",
    "print(model)\n",
    "model.load_state_dict(torch.load(param_path))\n",
    "loss_record = tester(test_loader, model, config, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
